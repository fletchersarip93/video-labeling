{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66c832-fe71-4393-a666-aacb087ad3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497bf53-5cc6-4cf5-9ee3-8de1f07265ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the cv2.VideoCapture handle. Remember to release the handle once you are done.\n",
    "def open_video(video_filepath):\n",
    "    vid_cap = cv2.VideoCapture(video_filepath)\n",
    "    \n",
    "    if not vid_cap.isOpened():\n",
    "        raise Exception('Error opening video {video_filepath}')\n",
    "        \n",
    "    return vid_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aacb30-594e-41a9-bf0e-dcd5fa2ef6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_frame_index(video_filepath, event_timestamp_millis):\n",
    "    vid_cap = open_video(video_filepath)\n",
    "    vid_fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "    vid_cap.release()\n",
    "    \n",
    "    return math.floor(event_timestamp_millis / 1000 * vid_fps) # todo: revisit this if there is issue with indexing too early\n",
    "\n",
    "def get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis, sequence_length, frame_step):\n",
    "    vid_cap = open_video(video_filepath)\n",
    "    vid_frame_count = vid_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    vid_cap.release()\n",
    "    \n",
    "    event_frame_index = get_event_frame_index(video_filepath, event_timestamp_millis)\n",
    "    \n",
    "    max_frame_steps_event_to_beginning = math.floor(event_frame_index / frame_step)\n",
    "    max_frame_steps_event_to_end = math.floor((vid_frame_count - 1 - event_frame_index)/ frame_step) # reason for minus 1 is this is about index and not about frame count\n",
    "    max_possible_sequence_length = max_frame_steps_event_to_end + max_frame_steps_event_to_beginning + 1\n",
    "    \n",
    "    # plus 1 to include the event frame itself\n",
    "    if sequence_length > max_possible_sequence_length:\n",
    "        raise Exception(f\"Not possible for frame step {frame_step} and sequence length {sequence_length}. Maximum possible sequence length is {max_possible_sequence_length}\")\n",
    "\n",
    "    # Min and max start frame in which the event frame is still included at the exact point\n",
    "    # considering the requested sequence length and frame step size.\n",
    "    min_start_frame_idx = event_frame_index - frame_step * min(sequence_length - 1, max_frame_steps_event_to_beginning)\n",
    "    max_start_frame_idx = event_frame_index - frame_step * max(0, sequence_length - 1 - max_frame_steps_event_to_end)\n",
    "\n",
    "    frame_indexes = []\n",
    "    \n",
    "    for start_frame_idx in range(min_start_frame_idx, (max_start_frame_idx + frame_step), frame_step):\n",
    "        # generate the index\n",
    "        frame_indexes.append([i for i in range(start_frame_idx, start_frame_idx + sequence_length * frame_step, frame_step)])\n",
    "\n",
    "    frame_indexes = np.array(frame_indexes)\n",
    "    labels = np.array(frame_indexes >= event_frame_index, dtype=np.int8)\n",
    "    \n",
    "    return frame_indexes, labels\n",
    "\n",
    "# test\n",
    "video_filepath = 'data/hand_collission.mp4'\n",
    "\n",
    "sample_result = get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis=1000, sequence_length=6, frame_step=13)\n",
    "assert sample_result[0].shape == (3, 6)\n",
    "\n",
    "if DEBUG:\n",
    "    print(f'Data shape: {sample_result[0].shape}')\n",
    "    print(f'Label shape: {sample_result[1].shape}')\n",
    "    print(sample_result)\n",
    "\n",
    "try:\n",
    "    get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis=1000, sequence_length=14, frame_step=13)\n",
    "    assert False # shouldn't get to this code as we expect exception to be thrown\n",
    "except Exception as exc:\n",
    "    if DEBUG:\n",
    "        print(f'Received expected exception with message \"{exc}\".')\n",
    "\n",
    "sample_result = get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis=1000, sequence_length=8, frame_step=13)\n",
    "assert sample_result is not None\n",
    "\n",
    "if DEBUG:\n",
    "    print(f'Data shape: {sample_result[0].shape}')\n",
    "    print(f'Label shape: {sample_result[1].shape}')\n",
    "    print(sample_result)\n",
    "    \n",
    "print('All tests OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d93a18-f04c-4f16-a226-e8b8badab94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will get image frames from the given video file path,\n",
    "# for the requested image frame indexes.\n",
    "# You can request several image frame index sequences.\n",
    "# Each row in the frame index sequence array corresponds to each sequence.\n",
    "# The returned image frames will be in form of numpy,\n",
    "# The numpy array will be arranged following the requested image frame index sequences.\n",
    "# Returned images will be in RGB format.\n",
    "# The numpy arrays for the image frames are read-only.\n",
    "# The same image frame will share the same memory location, even though they appear in multiple sequences.\n",
    "def get_image_frames(video_filepath, frame_index_sequences):\n",
    "    vid_cap = open_video(video_filepath)\n",
    "    \n",
    "    unique_frame_indexes = set([idx for sequence in frame_index_sequences for idx in sequence])\n",
    "    \n",
    "    frames_map = {}\n",
    "    \n",
    "    for frame_idx in unique_frame_indexes:\n",
    "        ret = vid_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        if not ret:\n",
    "            raise Exception(f'Failed to set the frame position for the VideoCapture.')\n",
    "        \n",
    "        ret, frame = vid_cap.read()\n",
    "        if not ret:\n",
    "            raise Exception(f'Failed to read image frame index {frame_idx}.')\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        frame = np.array(frame)\n",
    "        \n",
    "        # Set the numpy array to be read only, because we want this same array to be referenced\n",
    "        # in multiple location in the returned array.\n",
    "        frame.flags.writeable = False\n",
    "        \n",
    "        frames_map[frame_idx] = frame\n",
    "        \n",
    "    vid_cap.release()\n",
    "    \n",
    "    vid_frames = [np.array([frames_map[frame_idx] for frame_idx in sequence]) for sequence in frame_index_sequences]\n",
    "    \n",
    "    return vid_frames\n",
    "\n",
    "# test\n",
    "video_filepath = 'data/hand_collission.mp4'\n",
    "\n",
    "result = get_image_frames(video_filepath, [[1,2,3], [2,3,4,5]])\n",
    "assert result[0].shape == (3, 720, 1280, 3)\n",
    "assert result[1].shape == (4, 720, 1280, 3)\n",
    "\n",
    "if DEBUG:\n",
    "    print(result)\n",
    "\n",
    "print('All tests OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac97530-c5d0-4643-98f5-6e02aff7385a",
   "metadata": {},
   "source": [
    "Below I'll show the GIF of the filtered frames. I simulate a \"collision\" of my hands in the test video that I'm using. My hands collide at around timestamp 1500 msec. The code then extract a set of frames which include the exact frame at timestamp 1500 msec so we have the frame at the exact moment when my hand \"collided\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a3b82-86ef-426f-ba64-03e224ef9c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example frames in GIF\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "from pathlib import Path\n",
    "\n",
    "tmp_dir = './tmp'\n",
    "Path(tmp_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "video_filepath = 'data/hand_collission.mp4'\n",
    "gif_out_file_path = f'{tmp_dir}/test.gif'\n",
    "\n",
    "frame_index_sequences, labels = get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis=1500, sequence_length=11, frame_step=10)\n",
    "vid_frames = get_image_frames(video_filepath, frame_index_sequences)\n",
    "\n",
    "imageio.mimsave(gif_out_file_path, vid_frames[0], fps=10)\n",
    "Image(filename=gif_out_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53441590-66cd-4b84-8628-28541aa8be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_proportion_of_ones(labels, min_proportion_of_ones, max_proportion_of_ones):\n",
    "    proportion_of_ones = np.sum(labels, axis=-1) / labels.shape[-1]\n",
    "    \n",
    "    return (min_proportion_of_ones <= proportion_of_ones) & (proportion_of_ones <= max_proportion_of_ones)\n",
    "\n",
    "# test\n",
    "video_filepath = 'data/hand_collission.mp4'\n",
    "frame_index_sequences, labels = get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis=1500, sequence_length=7, frame_step=10)\n",
    "indexes = get_index_proportion_of_ones(labels, 0.5, 0.8)\n",
    "\n",
    "assert frame_index_sequences[indexes].shape == (2, 7)\n",
    "assert labels[indexes].shape == (2, 7)\n",
    "\n",
    "if DEBUG:\n",
    "    print(frame_index_sequences[indexes])\n",
    "    print(labels[indexes])\n",
    "\n",
    "print('All tests OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19329b37-7dc5-4177-8799-fc9369994fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames_surrounding_event(video_filepath, event_timestamp_millis, sequence_length, frame_step, min_proportion_of_after_event_frames, max_proportion_of_after_event_frames):\n",
    "    frame_idxs, labels = get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis, sequence_length, frame_step)\n",
    "    filter_idxs = get_index_proportion_of_ones(labels, min_proportion_of_after_event_frames, max_proportion_of_after_event_frames)\n",
    "    frame_idxs = frame_idxs[filter_idxs]\n",
    "    labels = labels[filter_idxs]\n",
    "    frame_imgs = get_image_frames(video_filepath, frame_idxs)\n",
    "    frame_imgs = np.array(frame_imgs) # convert to numpy array since the shape is homogeneous\n",
    "    \n",
    "    return frame_imgs, labels\n",
    "\n",
    "# test\n",
    "video_filepath = 'data/hand_collission.mp4'\n",
    "frame_imgs, labels = get_frames_surrounding_event(video_filepath,\n",
    "                                                  event_timestamp_millis=1500,\n",
    "                                                  sequence_length=15,\n",
    "                                                  frame_step=5,\n",
    "                                                  min_proportion_of_after_event_frames=0.4,\n",
    "                                                  max_proportion_of_after_event_frames=0.6)\n",
    "\n",
    "assert frame_imgs.shape == (3, 15, 720, 1280, 3)\n",
    "assert labels.shape == (3, 15)\n",
    "\n",
    "if DEBUG:\n",
    "    print('frame_imgs.shape:')\n",
    "    print(frame_imgs.shape)\n",
    "    print('labels.shape:')\n",
    "    print(labels.shape)\n",
    "    print('labels:')\n",
    "    print(labels)\n",
    "    \n",
    "print('All tests OK.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
