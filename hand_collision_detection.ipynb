{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66c832-fe71-4393-a666-aacb087ad3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import einops\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497bf53-5cc6-4cf5-9ee3-8de1f07265ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the cv2.VideoCapture handle. Remember to release the handle once you are done.\n",
    "def open_video(video_filepath):\n",
    "    vid_cap = cv2.VideoCapture(video_filepath)\n",
    "    \n",
    "    if not vid_cap.isOpened():\n",
    "        raise Exception(f'Error opening video {video_filepath}')\n",
    "        \n",
    "    return vid_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aacb30-594e-41a9-bf0e-dcd5fa2ef6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_frame_index(video_filepath, event_timestamp_millis):\n",
    "    vid_cap = open_video(video_filepath)\n",
    "    vid_fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "    vid_cap.release()\n",
    "    \n",
    "    return math.floor(event_timestamp_millis / 1000 * vid_fps) # todo: revisit this if there is issue with indexing too early\n",
    "\n",
    "def get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis, sequence_length, frame_step):\n",
    "    vid_cap = open_video(video_filepath)\n",
    "    vid_frame_count = vid_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    vid_cap.release()\n",
    "    \n",
    "    event_frame_index = get_event_frame_index(video_filepath, event_timestamp_millis)\n",
    "    \n",
    "    max_frame_steps_event_to_beginning = math.floor(event_frame_index / frame_step)\n",
    "    max_frame_steps_event_to_end = math.floor((vid_frame_count - 1 - event_frame_index)/ frame_step) # reason for minus 1 is this is about index and not about frame count\n",
    "    \n",
    "    max_possible_sequence_length = max_frame_steps_event_to_end + max_frame_steps_event_to_beginning + 1\n",
    "    \n",
    "    # plus 1 to include the event frame itself\n",
    "    if sequence_length > max_possible_sequence_length:\n",
    "        raise Exception(f\"Not possible for frame step {frame_step} and sequence length {sequence_length}. Maximum possible sequence length is {max_possible_sequence_length}\")\n",
    "\n",
    "    # Min and max start frame in which the event frame is still included at the exact point\n",
    "    # considering the requested sequence length and frame step size.\n",
    "    min_start_frame_idx = event_frame_index - frame_step * min(sequence_length - 1, max_frame_steps_event_to_beginning)\n",
    "    max_start_frame_idx = event_frame_index - frame_step * max(0, sequence_length - 1 - max_frame_steps_event_to_end)\n",
    "\n",
    "    frame_indexes = []\n",
    "    \n",
    "    for start_frame_idx in range(min_start_frame_idx, (max_start_frame_idx + frame_step), frame_step):\n",
    "        # generate the index\n",
    "        frame_indexes.append([i for i in range(start_frame_idx, start_frame_idx + sequence_length * frame_step, frame_step)])\n",
    "\n",
    "    frame_indexes = np.array(frame_indexes)\n",
    "    labels = np.array(frame_indexes >= event_frame_index, dtype=np.int8)\n",
    "    \n",
    "    return frame_indexes, labels\n",
    "\n",
    "# test\n",
    "video_filepath = 'data/hand_collision_videos/hand_collision.mp4'\n",
    "\n",
    "sample_result = get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis=1000, sequence_length=6, frame_step=13)\n",
    "assert sample_result[0].shape == (3, 6)\n",
    "\n",
    "if DEBUG:\n",
    "    print(f'Data shape: {sample_result[0].shape}')\n",
    "    print(f'Label shape: {sample_result[1].shape}')\n",
    "    print(sample_result)\n",
    "\n",
    "try:\n",
    "    get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis=1000, sequence_length=14, frame_step=13)\n",
    "    assert False # shouldn't get to this code as we expect exception to be thrown\n",
    "except Exception as exc:\n",
    "    if DEBUG:\n",
    "        print(f'Received expected exception with message \"{exc}\".')\n",
    "\n",
    "sample_result = get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis=1000, sequence_length=8, frame_step=13)\n",
    "assert sample_result is not None\n",
    "\n",
    "if DEBUG:\n",
    "    print(f'Data shape: {sample_result[0].shape}')\n",
    "    print(f'Label shape: {sample_result[1].shape}')\n",
    "    print(sample_result)\n",
    "    \n",
    "print('All tests OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d93a18-f04c-4f16-a226-e8b8badab94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will get image frames from the given video file path,\n",
    "# for the requested image frame indexes.\n",
    "# You can request several image frame index sequences.\n",
    "# Each row in the frame index sequence array corresponds to each sequence.\n",
    "# The returned image frames will be in form of numpy,\n",
    "# The numpy array will be arranged following the requested image frame index sequences.\n",
    "# Returned images will be in RGB format.\n",
    "# The numpy arrays for the image frames are read-only.\n",
    "# The same image frame will share the same memory location, even though they appear in multiple sequences.\n",
    "# The input argument `frame_index_sequences` should be a list of list, e.g. [[2,3,4], [1,2,3]]\n",
    "# The returned value will be a python list instead of a numpy, to cater for the case of non-homogeneous array.\n",
    "def get_image_frames(video_filepath, frame_index_sequences, format_frame_fn=None):\n",
    "    vid_cap = open_video(video_filepath)\n",
    "    \n",
    "    unique_frame_indexes = set([idx for sequence in frame_index_sequences for idx in sequence])\n",
    "    \n",
    "    frames_map = {}\n",
    "    \n",
    "    for frame_idx in unique_frame_indexes:\n",
    "        ret = vid_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        if not ret:\n",
    "            raise Exception(f'Failed to set the frame position for the VideoCapture.')\n",
    "        \n",
    "        ret, frame = vid_cap.read()\n",
    "        if not ret:\n",
    "            raise Exception(f'Failed to read image frame index {frame_idx}.')\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        frame = np.array(frame)\n",
    "        \n",
    "        if format_frame_fn is not None:\n",
    "            frame = format_frame_fn(frame)\n",
    "        \n",
    "        # Set the numpy array to be read only, because we want this same array to be referenced\n",
    "        # in multiple location in the returned array.\n",
    "        frame.flags.writeable = False\n",
    "        \n",
    "        frames_map[frame_idx] = frame\n",
    "        \n",
    "    vid_cap.release()\n",
    "    \n",
    "    vid_frames = [np.array([frames_map[frame_idx] for frame_idx in sequence]) for sequence in frame_index_sequences]\n",
    "    \n",
    "    return vid_frames\n",
    "\n",
    "# test\n",
    "video_filepath = 'data/hand_collision_videos/hand_collision.mp4'\n",
    "\n",
    "result = get_image_frames(video_filepath, [[1,2,3], [2,3,4,5]])\n",
    "assert result[0].shape == (3, 720, 1280, 3)\n",
    "assert result[1].shape == (4, 720, 1280, 3)\n",
    "\n",
    "if DEBUG:\n",
    "    print(result)\n",
    "\n",
    "print('All tests OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac97530-c5d0-4643-98f5-6e02aff7385a",
   "metadata": {},
   "source": [
    "Below I'll show the GIF of the filtered frames. I simulate a \"collision\" of my hands in the test video that I'm using. My hands collide at around timestamp 1500 msec. The code then extract a set of frames which include the exact frame at timestamp 1500 msec so we have the frame at the exact moment when my hand \"collided\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a3b82-86ef-426f-ba64-03e224ef9c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example frames in GIF\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "\n",
    "tmp_dir = './tmp'\n",
    "pathlib.Path(tmp_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "video_filepath = 'data/hand_collision_videos/hand_collision.mp4'\n",
    "gif_out_file_path = f'{tmp_dir}/test.gif'\n",
    "\n",
    "frame_index_sequences, labels = get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis=1500, sequence_length=11, frame_step=10)\n",
    "vid_frames = get_image_frames(video_filepath, frame_index_sequences)\n",
    "\n",
    "imageio.mimsave(gif_out_file_path, vid_frames[0], fps=10)\n",
    "Image(filename=gif_out_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53441590-66cd-4b84-8628-28541aa8be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_proportion_of_ones(labels, min_proportion_of_ones, max_proportion_of_ones):\n",
    "    proportion_of_ones = np.sum(labels, axis=-1) / labels.shape[-1]\n",
    "    \n",
    "    return (min_proportion_of_ones <= proportion_of_ones) & (proportion_of_ones <= max_proportion_of_ones)\n",
    "\n",
    "# test\n",
    "video_filepath = 'data/hand_collision_videos/hand_collision.mp4'\n",
    "frame_index_sequences, labels = get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis=1500, sequence_length=7, frame_step=10)\n",
    "indexes = get_index_proportion_of_ones(labels, 0.5, 0.8)\n",
    "\n",
    "assert frame_index_sequences[indexes].shape == (2, 7)\n",
    "assert labels[indexes].shape == (2, 7)\n",
    "\n",
    "if DEBUG:\n",
    "    print(frame_index_sequences[indexes])\n",
    "    print(labels[indexes])\n",
    "\n",
    "print('All tests OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19329b37-7dc5-4177-8799-fc9369994fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames_surrounding_event(video_filepath, event_timestamp_millis, sequence_length, frame_step, min_proportion_of_after_event_frames, max_proportion_of_after_event_frames, format_frame_fn=None):\n",
    "    frame_idxs, labels = get_frame_indexes_surrounding_event(video_filepath, event_timestamp_millis, sequence_length, frame_step)\n",
    "    filter_idxs = get_index_proportion_of_ones(labels, min_proportion_of_after_event_frames, max_proportion_of_after_event_frames)\n",
    "    frame_idxs = frame_idxs[filter_idxs]\n",
    "    labels = labels[filter_idxs]\n",
    "    frame_imgs = get_image_frames(video_filepath, frame_idxs, format_frame_fn=format_frame_fn)\n",
    "    frame_imgs = np.array(frame_imgs) # convert to numpy array since the shape is homogeneous\n",
    "    \n",
    "    return frame_imgs, labels\n",
    "\n",
    "# test\n",
    "video_filepath = 'data/hand_collision_videos/hand_collision.mp4'\n",
    "frame_imgs, labels = get_frames_surrounding_event(video_filepath,\n",
    "                                                  event_timestamp_millis=1500,\n",
    "                                                  sequence_length=15,\n",
    "                                                  frame_step=5,\n",
    "                                                  min_proportion_of_after_event_frames=0.4,\n",
    "                                                  max_proportion_of_after_event_frames=0.6)\n",
    "\n",
    "assert frame_imgs.shape == (3, 15, 720, 1280, 3)\n",
    "assert labels.shape == (3, 15)\n",
    "\n",
    "if DEBUG:\n",
    "    print('frame_imgs.shape:')\n",
    "    print(frame_imgs.shape)\n",
    "    print('labels.shape:')\n",
    "    print(labels.shape)\n",
    "    print('labels:')\n",
    "    print(labels)\n",
    "    \n",
    "print('All tests OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550b254-3261-49d6-a1b9-79540102a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sequence(seq_length, step_size, total_frames):\n",
    "    total_seq_frames = step_size * seq_length\n",
    "\n",
    "    if total_seq_frames > total_frames:\n",
    "        raise Exception(f'Number of video frames ({total_frames}) is not enough for the requested total sequence frames ({total_seq_frames})')\n",
    "\n",
    "    max_allowed_offset = total_frames - total_seq_frames\n",
    "\n",
    "    start_index = random.randint(0, max_allowed_offset)\n",
    "    end_index = start_index + seq_length * step_size\n",
    "\n",
    "    return list(range(start_index, end_index, step_size))\n",
    "\n",
    "# test that no error happens after 100000 sampling of random sequences\n",
    "for i in range(100000):\n",
    "    seq_length = 13\n",
    "    result = get_random_sequence(seq_length=seq_length, step_size=33, total_frames=774)\n",
    "    assert len(result) == seq_length\n",
    "    \n",
    "print('All tests OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e677bc-5aa7-4788-b9b6-46d619e2d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_video_frames(video_filepath, seq_length, step_size, num_seqs=1, format_frame_fn=None):\n",
    "    vid_cap = open_video(video_filepath)\n",
    "    vid_frame_count = vid_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    vid_cap.release()\n",
    "    \n",
    "    seq_indexes = []\n",
    "    for i in range(num_seqs):\n",
    "        seq_indexes.append(get_random_sequence(seq_length=seq_length, step_size=step_size, total_frames=vid_frame_count))\n",
    "    \n",
    "    # return as numpy array because we are sure that the array is homogeneous\n",
    "    return np.array(get_image_frames(video_filepath, seq_indexes, format_frame_fn))\n",
    "\n",
    "# test\n",
    "video_filepath = 'data/hand_collision_videos/hand_collision.mp4'\n",
    "num_seqs = 4\n",
    "\n",
    "result = get_random_video_frames(video_filepath=video_filepath, seq_length=13, step_size=5, num_seqs=num_seqs)\n",
    "assert result.shape == (num_seqs, 13, 720, 1280, 3)\n",
    "\n",
    "print('All tests OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54136a5-1351-4822-9d9a-05ca807a6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameGenerator:\n",
    "    def __init__(self, videos_dir_path, labels_file_path, sequence_length, frame_step_size,\n",
    "                 min_proportion_of_after_event_frames, max_proportion_of_after_event_frames, num_sequences_for_no_event_videos,\n",
    "                 format_frame_fn=None):\n",
    "        self.videos_dir_path = videos_dir_path\n",
    "        self.labels_file_path = labels_file_path\n",
    "        self.sequence_length = sequence_length\n",
    "        self.frame_step_size = frame_step_size\n",
    "        self.min_proportion_of_after_event_frames = min_proportion_of_after_event_frames\n",
    "        self.max_proportion_of_after_event_frames = max_proportion_of_after_event_frames\n",
    "        self.num_sequences_for_no_event_videos = num_sequences_for_no_event_videos\n",
    "        self.format_frame_fn = format_frame_fn\n",
    "    \n",
    "    def __call__(self):\n",
    "        labels_df = pd.read_csv(labels_file_path)\n",
    "        \n",
    "        videos_dir = pathlib.Path(videos_dir_path)\n",
    "        \n",
    "        for _, labels_row in labels_df.iterrows():\n",
    "            video_filename = labels_row['video_filename']\n",
    "            video_filepath = videos_dir / video_filename\n",
    "            video_filepath = str(video_filepath.resolve())\n",
    "            \n",
    "            event_timestamp_millis = labels_row['event_timestamp_millis']\n",
    "            \n",
    "            frames_seqs = []\n",
    "            labels = []\n",
    "            \n",
    "            if event_timestamp_millis >= 0:\n",
    "                frames_seqs, labels = get_frames_surrounding_event(video_filepath=video_filepath,\n",
    "                                                                   event_timestamp_millis=event_timestamp_millis,\n",
    "                                                                   sequence_length=self.sequence_length,\n",
    "                                                                   frame_step=self.frame_step_size,\n",
    "                                                                   min_proportion_of_after_event_frames=self.min_proportion_of_after_event_frames,\n",
    "                                                                   max_proportion_of_after_event_frames=self.max_proportion_of_after_event_frames,\n",
    "                                                                   format_frame_fn=self.format_frame_fn)\n",
    "            else:\n",
    "                frames_seqs = get_random_video_frames(video_filepath,\n",
    "                                                      seq_length=self.sequence_length,\n",
    "                                                      step_size=self.frame_step_size,\n",
    "                                                      num_seqs=self.num_sequences_for_no_event_videos,\n",
    "                                                      format_frame_fn=self.format_frame_fn)\n",
    "                \n",
    "                labels = np.zeros(shape=(self.num_sequences_for_no_event_videos, self.sequence_length))\n",
    "            \n",
    "            for (frames, label) in zip(frames_seqs, labels):\n",
    "                yield frames, label.any().astype(np.uint8)\n",
    "                \n",
    "            \n",
    "\n",
    "# test\n",
    "labels_file_path = './labels.csv'\n",
    "videos_dir_path = './data/hand_collision_videos'\n",
    "\n",
    "frame_generator = FrameGenerator(videos_dir_path=videos_dir_path, labels_file_path=labels_file_path, sequence_length=15,\n",
    "                                 frame_step_size=5, min_proportion_of_after_event_frames=0.3, max_proportion_of_after_event_frames=0.8,\n",
    "                                 num_sequences_for_no_event_videos=5)\n",
    "\n",
    "for frames, label in frame_generator():\n",
    "    assert frames.shape == (15, 720, 1280, 3)\n",
    "    assert frames.dtype == np.uint8\n",
    "    assert label.shape == ()\n",
    "    assert label.dtype == np.uint8\n",
    "    if DEBUG:\n",
    "        print(frames.shape)\n",
    "        print(frames.dtype)\n",
    "        print(label.shape)\n",
    "        print(label.dtype)\n",
    "        print(label)\n",
    "\n",
    "print('All tests OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af2a9b-fbb7-4ba2-8675-8286dcf5fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format image frame before input to model\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "\n",
    "def format_image_frame(frame):\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    frame = tf.image.resize_with_pad(frame, HEIGHT, WIDTH)\n",
    "    \n",
    "    return frame.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d453b6a-439c-4edc-89a4-35633a726ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "labels_file_path = './labels.csv'\n",
    "videos_dir_path = './data/hand_collision_videos'\n",
    "batch_size = 20\n",
    "sequence_length = 15\n",
    "frame_step_size = 5\n",
    "min_proportion_of_after_event_frames = 0.3\n",
    "max_proportion_of_after_event_frames = 0.8\n",
    "num_sequences_for_no_event_videos = 7\n",
    "\n",
    "frame_generator = FrameGenerator(videos_dir_path=videos_dir_path,\n",
    "                                 labels_file_path=labels_file_path,\n",
    "                                 sequence_length=sequence_length,\n",
    "                                 frame_step_size=frame_step_size,\n",
    "                                 min_proportion_of_after_event_frames=min_proportion_of_after_event_frames,\n",
    "                                 max_proportion_of_after_event_frames=max_proportion_of_after_event_frames,\n",
    "                                 num_sequences_for_no_event_videos=num_sequences_for_no_event_videos,\n",
    "                                 format_frame_fn=format_image_frame)\n",
    "\n",
    "output_signature = (tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32), tf.TensorSpec(shape=(), dtype=tf.uint8))\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(frame_generator, output_signature=output_signature)\n",
    "\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE).cache().shuffle(buffer_size=1000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n",
    "\n",
    "# test\n",
    "ratio_of_ones = []\n",
    "\n",
    "for frames, labels in train_ds.take(10):\n",
    "    assert frames.shape == (batch_size, sequence_length, HEIGHT, WIDTH, 3)\n",
    "    assert labels.shape == (batch_size,)\n",
    "    ratio_of_ones.append(np.mean(labels))\n",
    "\n",
    "print('All tests OK.')\n",
    "print(f'Class balance evaluation: {np.mean(ratio_of_ones)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91355bc1-bc65-4cba-8479-c1df5869eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Plus1D(keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.seq = keras.Sequential([\n",
    "            keras.layers.Conv3D(filters=filters,\n",
    "                          kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                          padding=padding),\n",
    "            keras.layers.Conv3D(filters=filters,\n",
    "                          kernel_size=(kernel_size[0], 1, 1),\n",
    "                          padding=padding)\n",
    "        ])\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.seq(x)\n",
    "    \n",
    "class ResidualMain(keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size):\n",
    "        super().__init__()\n",
    "        self.seq = keras.Sequential([\n",
    "            Conv2Plus1D(filters=filters,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding='same'),\n",
    "            keras.layers.LayerNormalization(),\n",
    "            keras.layers.ReLU(),\n",
    "            Conv2Plus1D(filters=filters,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding='same'),\n",
    "            keras.layers.LayerNormalization()\n",
    "        ])\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "class Project(keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.seq = keras.Sequential([\n",
    "            keras.layers.Dense(units),\n",
    "            keras.layers.LayerNormalization()\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.seq(x)\n",
    "    \n",
    "def add_residual_block(input, filters, kernel_size):\n",
    "    out = ResidualMain(filters=filters, kernel_size=kernel_size)(input)\n",
    "    \n",
    "    res = input\n",
    "    if out.shape[-1] != input.shape[-1]:\n",
    "        res = Project(out.shape[-1])(res)\n",
    "    \n",
    "    return keras.layers.add([res, out])\n",
    "\n",
    "class ResizeVideo(keras.layers.Layer):\n",
    "    def __init__(self, height, width):\n",
    "        super().__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.resizing_layer = keras.layers.Resizing(self.height, self.width)\n",
    "        \n",
    "    def call(self, video):\n",
    "        old_shape = einops.parse_shape(video, 'b t h w c')\n",
    "        images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n",
    "        images = self.resizing_layer(images)\n",
    "        videos = einops.rearrange(images, '(b t) h w c -> b t h w c',\n",
    "                                  t=old_shape['t'])\n",
    "        \n",
    "        return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc11a3e-1a9d-4f1b-b3c5-ec70867378c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "input = layers.Input(shape=(sequence_length, HEIGHT, WIDTH, 3))\n",
    "x = input\n",
    "x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = ResizeVideo(HEIGHT//2, WIDTH//2)(x)\n",
    "\n",
    "x = add_residual_block(x, filters=16, kernel_size=(3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT//4, WIDTH//4)(x)\n",
    "\n",
    "x = add_residual_block(x, filters=32, kernel_size=(3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT//8, WIDTH//8)(x)\n",
    "\n",
    "x = add_residual_block(x, filters=64, kernel_size=(3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT//16, WIDTH//16)(x)\n",
    "\n",
    "x = add_residual_block(x, filters=128, kernel_size=(3, 3, 3))\n",
    "\n",
    "x = keras.layers.GlobalAveragePooling3D()(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "\n",
    "model = keras.Model(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e333934-0aa2-4646-b6d0-19c455b8c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56e1e7-5b4f-41a7-9cea-43bb6149646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0fdee5-8bbe-4d25-95c4-e53bf4031320",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./training_checkpoint/checkpoint\"\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor='loss',\n",
    "                                                 mode='min',\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "history = model.fit(x=train_ds,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=50,\n",
    "                    callbacks=[cp_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
